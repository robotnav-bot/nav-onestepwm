<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="An Efficient and Multi-Modal Navigation System with One-Step World Model.">
  <meta name="keywords" content="Navigation, World Models, Robotics, 3D U-Net, Diffusion Policies">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>An Efficient and Multi-Modal Navigation System with One-Step World Model</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://scholar.google.com/citations?user=fy3BKH4AAAAJ&hl">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div> -->
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">An Efficient and Multi-Modal Navigation System with One-Step World Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=fy3BKH4AAAAJ&hl">Wangtian Shen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=70Fly6cAAAAJ">Ziyang Meng</a><sup>1 *</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=GSVkVZ0AAAAJ">Jinming Ma</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Mingliang Zhou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Diyun Xiang</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <p>
            <span class="author-block"><sup>1</sup>Department of Precision Instrument, Tsinghua University,</span>
            </p>
            <p>
            <span class="author-block"><sup>2</sup>Xiaomi Robotics Lab, Beijing, China</span>
            </p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2601.12277" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2601.12277" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-youtube"></i></span>
                  <span>Video</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://github.com/robotnav-bot/NOW" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>Data</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <img src="./static/images/task-demo-new.png" alt="Task Demo" style="width: 100%;">
          <h2 class="subtitle has-text-centered" style="margin-top: 15px;">
            Our world-model-based framework enables<strong> multi-modal goal-conditioned navigation</strong>, including image, language, and point goals.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models—which predict future observations conditioned on given actions—with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">System Architecture</h2>
        <div class="content has-text-justified">
          <!-- <p>
            The overall framework of our proposed method.
          </p> -->
        </div>
        <img src="./static/images/architecture.png" alt="Architecture" style="width: 100%; margin-top: 10px; margin-bottom: 20px;">
        <div class="content has-text-justified">
          <p>
            Our system introduces a shortcut-based one-step generation paradigm for navigation world model. Unlike traditional diffusion models that require expensive iterative denoising, our world model directly predicts a sequence of 11 future frames in a single step. We utilize a 3D U-Net backbone operating within a VAE latent space. To handle high-dimensional video data efficiently, we employ a hybrid CNN-Transformer architecture with decoupled spatial and temporal attention. Specifically, a window-based temporal attention mechanism allows the model to capture complex dynamics without the quadratic complexity of full global attention. We integrate this lightweight world model into a model-based planning framework using the Cross-Entropy Method (CEM). To ensure robust performance under limited computational budgets (small sample size), we propose an anchor-based initialization strategy. Instead of random sampling, this approach initializes candidate trajectories using fixed velocity priors, significantly improving the planner's efficiency and success rate across multi-modal tasks (Image, Language, and Point goals).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Simulation Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our method in Habitat-sim across different modalities: Image-Goal, Language-Goal, and Point-Goal navigation. The sequence below the video displays the world model's predictions corresponding to the current optimal planned trajectory.
          </p>
        </div>
      </div>
    </div>

    <h3 class="title is-4 has-text-centered">Image-Goal Navigation</h3>
    <div class="content has-text-justified">
      <p>
        To implement image-goal navigation, we employ LPIPS as loss function to score sampled trajectories. Drawing inspiration from prior vision-based approaches, we utilize a topological memory system to achieve long-horizon navigation. Specifically, the process initiates with the first observation. At each time step, a distance estimation network identifies the nearest node in the topological graph to estimate the current location. Subsequently, the image of the adjacent node is fed into the policy system as the immediate target. 
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/image/0.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/image/1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/image/2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/image/3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/image/4.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br>

    <h3 class="title is-4 has-text-centered">Language-Goal Navigation</h3>
    <div class="content has-text-justified">
      <p>
        To implement language-goal navigation, we employ SigLIP as the scoring function to evaluate sampled trajectories.
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/language/0.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/language/1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/language/2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/language/3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/language/4.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br>

    <h3 class="title is-4 has-text-centered">Point-Goal Navigation</h3>
    <div class="content has-text-justified">
      <p>
        To implement Point-Goal Navigation, we incorporate Depth-Anything to perform monocular depth estimation on the predicted future images. We formulate a composite planning loss function to evaluate candidate trajectories. Specifically, this loss is defined as the weighted sum of two terms: (1) the Euclidean distance between the terminal point of the sampled trajectory and the target coordinate, and (2) the negative mean depth value in the central region of the predicted image (to penalize proximity to obstacles).
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/point/0.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/point/1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/point/2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/point/3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-fifth">
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/sim/point/4.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Real-World Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We deploy our model on a physical robot to test its robustness in real-world environments.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-one-third">
        <h5 class="title is-5 has-text-centered">Image-Goal</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/real/image.mp4" type="video/mp4">
        </video>
      </div>

      <div class="column is-one-third">
        <h5 class="title is-5 has-text-centered">Language-Goal</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/real/language.mp4" type="video/mp4">
        </video>
      </div>

      <div class="column is-one-third">
        <h5 class="title is-5 has-text-centered">Point-Goal</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/real/point.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{shen2026efficientmultimodalnavigationonestep,
      title={An Efficient and Multi-Modal Navigation System with One-Step World Model}, 
      author={Wangtian Shen and Ziyang Meng and Jinming Ma and Mingliang Zhou and Diyun Xiang},
      year={2026},
      eprint={2601.12277},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2601.12277}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/pdf/2601.12277">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/robotnav-bot/NOW" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>